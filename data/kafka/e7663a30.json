{
  "project": "kafka",
  "jira_id": "4303",
  "commit": "e7663a30",
  "classification": {
    "singleLine": false
  },
  "patch": "diff --git a/clients/src/main/java/org/apache/kafka/clients/InFlightRequests.java b/clients/src/main/java/org/apache/kafka/clients/InFlightRequests.java\nindex 8de19ee4f1..91b9dba72e 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/InFlightRequests.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/InFlightRequests.java\n@@ -26,7 +26,7 @@ import java.util.Map;\n final class InFlightRequests {\n \n     private final int maxInFlightRequestsPerConnection;\n-    private final Map<String, Deque<ClientRequest>> requests = new HashMap<String, Deque<ClientRequest>>();\n+    private final Map<String, Deque<ClientRequest>> requests = new HashMap<>();\n \n     public InFlightRequests(int maxInFlightRequestsPerConnection) {\n         this.maxInFlightRequestsPerConnection = maxInFlightRequestsPerConnection;\n@@ -133,16 +133,18 @@ final class InFlightRequests {\n      * @return list of nodes\n      */\n     public List<String> getNodesWithTimedOutRequests(long now, int requestTimeout) {\n-        List<String> nodeIds = new LinkedList<String>();\n-        for (String nodeId : requests.keySet()) {\n-            if (inFlightRequestCount(nodeId) > 0) {\n-                ClientRequest request = requests.get(nodeId).peekLast();\n+        List<String> nodeIds = new LinkedList<>();\n+        for (Map.Entry<String, Deque<ClientRequest>> requestEntry : requests.entrySet()) {\n+            String nodeId = requestEntry.getKey();\n+            Deque<ClientRequest> deque = requestEntry.getValue();\n+\n+            if (!deque.isEmpty()) {\n+                ClientRequest request = deque.peekLast();\n                 long timeSinceSend = now - request.sendTimeMs();\n-                if (timeSinceSend > requestTimeout) {\n+                if (timeSinceSend > requestTimeout)\n                     nodeIds.add(nodeId);\n             }\n         }\n-        }\n \n         return nodeIds;\n     }\ndiff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java b/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java\nindex b2b4bf0d4e..b384211e3d 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java\n@@ -1025,12 +1025,6 @@ public class KafkaConsumer<K, V> implements Consumer<K, V> {\n         // send any new fetches (won't resend pending fetches)\n         fetcher.sendFetches();\n \n-        // if no fetches could be sent at the moment (which can happen if a partition leader is in the\n-        // blackout period following a disconnect, or if the partition leader is unknown), then we don't\n-        // block for longer than the retry backoff duration.\n-        if (!fetcher.hasInFlightFetches())\n-            timeout = Math.min(timeout, retryBackoffMs);\n-\n         long now = time.milliseconds();\n         long pollTimeout = Math.min(coordinator.timeToNextPoll(now), timeout);\n \n@@ -1039,7 +1033,7 @@ public class KafkaConsumer<K, V> implements Consumer<K, V> {\n             public boolean shouldBlock() {\n                 // since a fetch might be completed by the background thread, we need this poll condition\n                 // to ensure that we do not block unnecessarily in poll()\n-                return !fetcher.hasCompletedFetches() && fetcher.hasInFlightFetches();\n+                return !fetcher.hasCompletedFetches();\n             }\n         });\n \ndiff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java\nindex 21fe0b8adc..2495b23db4 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java\n@@ -47,6 +47,7 @@ import java.util.concurrent.atomic.AtomicBoolean;\n  */\n public class ConsumerNetworkClient implements Closeable {\n     private static final Logger log = LoggerFactory.getLogger(ConsumerNetworkClient.class);\n+    private static final long MAX_POLL_TIMEOUT_MS = 5000L;\n \n     // the mutable state of this class is protected by the object's monitor (excluding the wakeup\n     // flag and the request completion queue below).\n@@ -176,7 +177,7 @@ public class ConsumerNetworkClient implements Closeable {\n      */\n     public void poll(RequestFuture<?> future) {\n         while (!future.isDone())\n-            poll(Long.MAX_VALUE, time.milliseconds(), future);\n+            poll(MAX_POLL_TIMEOUT_MS, time.milliseconds(), future);\n     }\n \n     /**\n@@ -225,7 +226,10 @@ public class ConsumerNetworkClient implements Closeable {\n             // condition becomes satisfied after the call to shouldBlock() (because of a fired completion\n             // handler), the client will be woken up.\n             if (pollCondition == null || pollCondition.shouldBlock()) {\n-                client.poll(timeout, now);\n+                // if there are no requests in flight, do not block longer than the retry backoff\n+                if (client.inFlightRequestCount() == 0)\n+                    timeout = Math.min(timeout, retryBackoffMs);\n+                client.poll(Math.min(MAX_POLL_TIMEOUT_MS, timeout), now);\n                 now = time.milliseconds();\n             } else {\n                 client.poll(0, now);\ndiff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java\nindex 9e9ae925ea..bfc1a0b01f 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java\n@@ -17,6 +17,7 @@ import org.apache.kafka.clients.ClientResponse;\n import org.apache.kafka.clients.Metadata;\n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.apache.kafka.clients.consumer.NoOffsetForPartitionException;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n import org.apache.kafka.clients.consumer.OffsetOutOfRangeException;\n import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n import org.apache.kafka.common.Cluster;\n@@ -41,7 +42,6 @@ import org.apache.kafka.common.protocol.Errors;\n import org.apache.kafka.common.record.InvalidRecordException;\n import org.apache.kafka.common.record.LogEntry;\n import org.apache.kafka.common.record.MemoryRecords;\n-import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n import org.apache.kafka.common.record.Record;\n import org.apache.kafka.common.record.TimestampType;\n import org.apache.kafka.common.requests.FetchRequest;\n@@ -91,7 +91,6 @@ public class Fetcher<K, V> {\n     private final FetchManagerMetrics sensors;\n     private final SubscriptionState subscriptions;\n     private final ConcurrentLinkedQueue<CompletedFetch> completedFetches;\n-    private final AtomicInteger numInFlightFetches = new AtomicInteger(0);\n     private final Deserializer<K> keyDeserializer;\n     private final Deserializer<V> valueDeserializer;\n \n@@ -137,15 +136,6 @@ public class Fetcher<K, V> {\n         return !completedFetches.isEmpty();\n     }\n \n-    /**\n-     * Check whether there are in-flight fetches. This is used to avoid unnecessary blocking in\n-     * {@link ConsumerNetworkClient#poll(long)} if there are no fetches to wait for. This method is thread-safe.\n-     * @return true if there are, false otherwise\n-     */\n-    public boolean hasInFlightFetches() {\n-        return numInFlightFetches.get() > 0;\n-    }\n-\n     private boolean matchesRequestedPartitions(FetchRequest request, FetchResponse response) {\n         Set<TopicPartition> requestedPartitions = request.fetchData().keySet();\n         Set<TopicPartition> fetchedPartitions = response.responseData().keySet();\n@@ -161,13 +151,10 @@ public class Fetcher<K, V> {\n             final FetchRequest request = fetchEntry.getValue();\n             final Node fetchTarget = fetchEntry.getKey();\n \n-            numInFlightFetches.incrementAndGet();\n             client.send(fetchTarget, ApiKeys.FETCH, request)\n                     .addListener(new RequestFutureListener<ClientResponse>() {\n                         @Override\n                         public void onSuccess(ClientResponse resp) {\n-                            numInFlightFetches.decrementAndGet();\n-\n                             FetchResponse response = new FetchResponse(resp.responseBody());\n                             if (!matchesRequestedPartitions(request, response)) {\n                                 // obviously we expect the broker to always send us valid responses, so this check\n@@ -194,7 +181,6 @@ public class Fetcher<K, V> {\n \n                         @Override\n                         public void onFailure(RuntimeException e) {\n-                            numInFlightFetches.decrementAndGet();\n                             log.debug(\"Fetch request to {} failed\", fetchTarget, e);\n                         }\n                     });\n",
  "files": 4,
  "linesAdd": 17,
  "linesRem": 31,
  "failing_tests": [],
  "nb_test": 0,
  "nb_failure": 0,
  "nb_error": 0,
  "nb_skipped": 0
}