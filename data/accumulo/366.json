{
  "files": 9, 
  "singleLine": false, 
  "nb_error": 0, 
  "failing_tests": [], 
  "nb_test": 4, 
  "patch": "diff --git a/src/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java b/src/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java\nindex 6931ea8..f5bdd6b 100644\n--- a/src/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java\n+++ b/src/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java\n@@ -136,7 +136,7 @@ public class ZooCache {\n         }\n         log.warn(\"Zookeeper error, will retry\", e);\n       } catch (InterruptedException e) {\n-        log.warn(\"Zookeeper error, will retry\", e);\n+        log.info(\"Zookeeper error, will retry\", e);\n       } catch (ConcurrentModificationException e) {\n         log.debug(\"Zookeeper was modified, will retry\");\n       }\ndiff --git a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/ArticleExtractor.java b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/ArticleExtractor.java\nindex 54e47b6..06d1670 100644\n--- a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/ArticleExtractor.java\n+++ b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/ArticleExtractor.java\n@@ -16,6 +16,9 @@\n  */\n package org.apache.accumulo.examples.wikisearch.ingest;\n \n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n import java.io.Reader;\n import java.text.ParseException;\n import java.text.SimpleDateFormat;\n@@ -29,6 +32,7 @@ import javax.xml.stream.XMLStreamReader;\n \n import org.apache.accumulo.examples.wikisearch.normalizer.LcNoDiacriticsNormalizer;\n import org.apache.accumulo.examples.wikisearch.normalizer.NumberNormalizer;\n+import org.apache.hadoop.io.Writable;\n \n \n public class ArticleExtractor {\n@@ -37,13 +41,15 @@ public class ArticleExtractor {\n   private static NumberNormalizer nn = new NumberNormalizer();\n   private static LcNoDiacriticsNormalizer lcdn = new LcNoDiacriticsNormalizer();\n   \n-  public static class Article {\n+  public static class Article implements Writable {\n     int id;\n     String title;\n     long timestamp;\n     String comments;\n     String text;\n     \n+    public Article(){}\n+    \n     private Article(int id, String title, long timestamp, String comments, String text) {\n       super();\n       this.id = id;\n@@ -90,6 +96,24 @@ public class ArticleExtractor {\n       fields.put(\"COMMENTS\", lcdn.normalizeFieldValue(\"COMMENTS\", this.comments));\n       return fields;\n     }\n+\n+    @Override\n+    public void readFields(DataInput in) throws IOException {\n+      id = in.readInt();\n+      title = in.readUTF();\n+      timestamp = in.readLong();\n+      comments = in.readUTF();\n+      text = in.readUTF();\n+    }\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {\n+      out.writeInt(id);\n+      out.writeUTF(title);\n+      out.writeLong(timestamp);\n+      out.writeUTF(comments);\n+      out.writeUTF(text);\n+    }\n     \n   }\n   \ndiff --git a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaConfiguration.java b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaConfiguration.java\nindex d76d713..5a0aad4 100644\n--- a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaConfiguration.java\n+++ b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaConfiguration.java\n@@ -48,6 +48,11 @@ public class WikipediaConfiguration {\n \n   public final static String NUM_GROUPS = \"wikipedia.ingest.groups\";\n \n+  public final static String PARTITIONED_ARTICLES_DIRECTORY = \"wikipedia.partitioned.directory\";\n+  \n+  public final static String RUN_PARTITIONER = \"wikipedia.run.partitioner\";\n+  public final static String RUN_INGEST = \"wikipedia.run.ingest\";\n+  \n   \n   public static String getUser(Configuration conf) {\n     return conf.get(USER);\n@@ -117,6 +122,18 @@ public class WikipediaConfiguration {\n     return conf.getInt(NUM_GROUPS, 1);\n   }\n   \n+  public static Path getPartitionedArticlesPath(Configuration conf) {\n+    return new Path(conf.get(PARTITIONED_ARTICLES_DIRECTORY));\n+  }\n+  \n+  public static boolean runPartitioner(Configuration conf) {\n+    return conf.getBoolean(RUN_PARTITIONER, false);\n+  }\n+\n+  public static boolean runIngest(Configuration conf) {\n+    return conf.getBoolean(RUN_INGEST, true);\n+  }\n+\n   /**\n    * Helper method to get properties from Hadoop configuration\n    * \ndiff --git a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaInputFormat.java b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaInputFormat.java\nindex e8b8b52..dd2eeb9 100644\n--- a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaInputFormat.java\n+++ b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaInputFormat.java\n@@ -75,10 +75,14 @@ public class WikipediaInputFormat extends TextInputFormat {\n       Path file = new Path(in.readUTF());\n       long start = in.readLong();\n       long length = in.readLong();\n-      int numHosts = in.readInt();\n-      String[] hosts = new String[numHosts];\n-      for(int i = 0; i < numHosts; i++)\n-        hosts[i] = in.readUTF();\n+      String [] hosts = null;\n+      if(in.readBoolean())\n+      {\n+        int numHosts = in.readInt();\n+        hosts = new String[numHosts];\n+        for(int i = 0; i < numHosts; i++)\n+          hosts[i] = in.readUTF();\n+      }\n       fileSplit = new FileSplit(file, start, length, hosts);\n       partition = in.readInt();\n     }\n@@ -89,10 +93,17 @@ public class WikipediaInputFormat extends TextInputFormat {\n       out.writeLong(fileSplit.getStart());\n       out.writeLong(fileSplit.getLength());\n       String [] hosts = fileSplit.getLocations();\n-      out.writeInt(hosts.length);\n-      for(String host:hosts)\n+      if(hosts == null)\n+      {\n+        out.writeBoolean(false);\n+      }\n+      else\n+      {\n+        out.writeBoolean(true);\n+        out.writeInt(hosts.length);\n+        for(String host:hosts)\n         out.writeUTF(host);\n-      fileSplit.write(out);\n+      }\n       out.writeInt(partition);\n     }\n     \ndiff --git a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitionedIngester.java b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitionedIngester.java\nnew file mode 100644\nindex 0000000..e7493dc\n--- /dev/null\n+++ b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitionedIngester.java\n@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.accumulo.examples.wikisearch.ingest;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.EnumSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import org.apache.accumulo.core.client.AccumuloException;\n+import org.apache.accumulo.core.client.AccumuloSecurityException;\n+import org.apache.accumulo.core.client.Connector;\n+import org.apache.accumulo.core.client.IteratorSetting;\n+import org.apache.accumulo.core.client.IteratorSetting.Column;\n+import org.apache.accumulo.core.client.TableExistsException;\n+import org.apache.accumulo.core.client.TableNotFoundException;\n+import org.apache.accumulo.core.client.admin.TableOperations;\n+import org.apache.accumulo.core.client.mapreduce.AccumuloOutputFormat;\n+import org.apache.accumulo.core.data.Mutation;\n+import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n+import org.apache.accumulo.core.iterators.user.SummingCombiner;\n+import org.apache.accumulo.examples.wikisearch.ingest.ArticleExtractor.Article;\n+import org.apache.accumulo.examples.wikisearch.iterator.GlobalIndexUidCombiner;\n+import org.apache.accumulo.examples.wikisearch.iterator.TextIndexCombiner;\n+import org.apache.accumulo.examples.wikisearch.reader.AggregatingRecordReader;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+\n+public class WikipediaPartitionedIngester extends Configured implements Tool {\n+  \n+  public final static String INGEST_LANGUAGE = \"wikipedia.ingest_language\";\n+  public final static String SPLIT_FILE = \"wikipedia.split_file\";\n+  public final static String TABLE_NAME = \"wikipedia.table\";\n+  \n+  public static void main(String[] args) throws Exception {\n+    int res = ToolRunner.run(new Configuration(), new WikipediaPartitionedIngester(), args);\n+    System.exit(res);\n+  }\n+  \n+  private void createTables(TableOperations tops, String tableName) throws AccumuloException, AccumuloSecurityException, TableNotFoundException,\n+      TableExistsException {\n+    // Create the shard table\n+    String indexTableName = tableName + \"Index\";\n+    String reverseIndexTableName = tableName + \"ReverseIndex\";\n+    String metadataTableName = tableName + \"Metadata\";\n+    \n+    // create the shard table\n+    if (!tops.exists(tableName)) {\n+      // Set a text index combiner on the given field names. No combiner is set if the option is not supplied\n+      String textIndexFamilies = WikipediaMapper.TOKENS_FIELD_NAME;\n+      \n+      tops.create(tableName);\n+      if (textIndexFamilies.length() > 0) {\n+        System.out.println(\"Adding content combiner on the fields: \" + textIndexFamilies);\n+        \n+        IteratorSetting setting = new IteratorSetting(10, TextIndexCombiner.class);\n+        List<Column> columns = new ArrayList<Column>();\n+        for (String family : StringUtils.split(textIndexFamilies, ',')) {\n+          columns.add(new Column(\"fi\\0\" + family));\n+        }\n+        TextIndexCombiner.setColumns(setting, columns);\n+        TextIndexCombiner.setLossyness(setting, true);\n+        \n+        tops.attachIterator(tableName, setting, EnumSet.allOf(IteratorScope.class));\n+      }\n+      \n+      // Set the locality group for the full content column family\n+      tops.setLocalityGroups(tableName, Collections.singletonMap(\"WikipediaDocuments\", Collections.singleton(new Text(WikipediaMapper.DOCUMENT_COLUMN_FAMILY))));\n+      \n+    }\n+    \n+    if (!tops.exists(indexTableName)) {\n+      tops.create(indexTableName);\n+      // Add the UID combiner\n+      IteratorSetting setting = new IteratorSetting(19, \"UIDAggregator\", GlobalIndexUidCombiner.class);\n+      GlobalIndexUidCombiner.setCombineAllColumns(setting, true);\n+      GlobalIndexUidCombiner.setLossyness(setting, true);\n+      tops.attachIterator(indexTableName, setting, EnumSet.allOf(IteratorScope.class));\n+    }\n+    \n+    if (!tops.exists(reverseIndexTableName)) {\n+      tops.create(reverseIndexTableName);\n+      // Add the UID combiner\n+      IteratorSetting setting = new IteratorSetting(19, \"UIDAggregator\", GlobalIndexUidCombiner.class);\n+      GlobalIndexUidCombiner.setCombineAllColumns(setting, true);\n+      GlobalIndexUidCombiner.setLossyness(setting, true);\n+      tops.attachIterator(reverseIndexTableName, setting, EnumSet.allOf(IteratorScope.class));\n+    }\n+    \n+    if (!tops.exists(metadataTableName)) {\n+      // Add the SummingCombiner with VARLEN encoding for the frequency column\n+      tops.create(metadataTableName);\n+      IteratorSetting setting = new IteratorSetting(10, SummingCombiner.class);\n+      SummingCombiner.setColumns(setting, Collections.singletonList(new Column(\"f\")));\n+      SummingCombiner.setEncodingType(setting, SummingCombiner.Type.VARLEN);\n+      tops.attachIterator(metadataTableName, setting, EnumSet.allOf(IteratorScope.class));\n+    }\n+  }\n+  \n+  @Override\n+  public int run(String[] args) throws Exception {\n+    Configuration conf = getConf();\n+    if(WikipediaConfiguration.runPartitioner(conf))\n+    {\n+      int result = runPartitionerJob();\n+      if(result != 0)\n+        return result;\n+    }\n+    if(WikipediaConfiguration.runIngest(conf))\n+      return runIngestJob();\n+    return 0;\n+  }\n+  \n+  public int runPartitionerJob() throws Exception\n+  {\n+    Job partitionerJob = new Job(getConf(), \"Partition Wikipedia\");\n+    Configuration partitionerConf = partitionerJob.getConfiguration();\n+    partitionerConf.set(\"mapred.map.tasks.speculative.execution\", \"false\");\n+\n+    configurePartitionerJob(partitionerJob);\n+    \n+    List<Path> inputPaths = new ArrayList<Path>();\n+    SortedSet<String> languages = new TreeSet<String>();\n+    FileSystem fs = FileSystem.get(partitionerConf);\n+    Path parent = new Path(partitionerConf.get(\"wikipedia.input\"));\n+    listFiles(parent, fs, inputPaths, languages);\n+    \n+    System.out.println(\"Input files in \" + parent + \":\" + inputPaths.size());\n+    Path[] inputPathsArray = new Path[inputPaths.size()];\n+    inputPaths.toArray(inputPathsArray);\n+    \n+    System.out.println(\"Languages:\" + languages.size());\n+\n+    // setup input format\n+    \n+    WikipediaInputFormat.setInputPaths(partitionerJob, inputPathsArray);\n+    \n+    partitionerJob.setMapperClass(WikipediaPartitioner.class);\n+    partitionerJob.setNumReduceTasks(0);\n+\n+    // setup output format\n+    partitionerJob.setMapOutputKeyClass(Text.class);\n+    partitionerJob.setMapOutputValueClass(Article.class);\n+    partitionerJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n+    Path outputDir = WikipediaConfiguration.getPartitionedArticlesPath(partitionerConf);\n+    SequenceFileOutputFormat.setOutputPath(partitionerJob, outputDir);\n+    \n+    return partitionerJob.waitForCompletion(true) ? 0 : 1;\n+  }\n+  \n+  public int runIngestJob() throws Exception\n+  {\n+    Job ingestJob = new Job(getConf(), \"Ingest Partitioned Wikipedia\");\n+    Configuration ingestConf = ingestJob.getConfiguration();\n+    ingestConf.set(\"mapred.map.tasks.speculative.execution\", \"false\");\n+\n+    String tablename = WikipediaConfiguration.getTableName(ingestConf);\n+    \n+    String zookeepers = WikipediaConfiguration.getZookeepers(ingestConf);\n+    String instanceName = WikipediaConfiguration.getInstanceName(ingestConf);\n+    \n+    String user = WikipediaConfiguration.getUser(ingestConf);\n+    byte[] password = WikipediaConfiguration.getPassword(ingestConf);\n+    Connector connector = WikipediaConfiguration.getConnector(ingestConf);\n+    \n+    TableOperations tops = connector.tableOperations();\n+    \n+    createTables(tops, tablename);\n+    \n+    // setup input format\n+    ingestJob.setInputFormatClass(SequenceFileInputFormat.class);\n+    SequenceFileInputFormat.setInputPaths(ingestJob, WikipediaConfiguration.getPartitionedArticlesPath(ingestConf));\n+\n+    // setup output format\n+    ingestJob.setMapOutputKeyClass(Text.class);\n+    ingestJob.setMapOutputValueClass(Mutation.class);\n+    ingestJob.setOutputFormatClass(AccumuloOutputFormat.class);\n+    AccumuloOutputFormat.setOutputInfo(ingestJob.getConfiguration(), user, password, true, tablename);\n+    AccumuloOutputFormat.setZooKeeperInstance(ingestJob.getConfiguration(), instanceName, zookeepers);\n+    \n+    return ingestJob.waitForCompletion(true) ? 0 : 1;\n+  }\n+  \n+  public final static PathFilter partFilter = new PathFilter() {\n+    @Override\n+    public boolean accept(Path path) {\n+      return path.getName().startsWith(\"part\");\n+    };\n+  };\n+  \n+  protected void configurePartitionerJob(Job job) {\n+    Configuration conf = job.getConfiguration();\n+    job.setJarByClass(WikipediaPartitionedIngester.class);\n+    job.setInputFormatClass(WikipediaInputFormat.class);\n+    conf.set(AggregatingRecordReader.START_TOKEN, \"<page>\");\n+    conf.set(AggregatingRecordReader.END_TOKEN, \"</page>\");\n+  }\n+  \n+  protected static final Pattern filePattern = Pattern.compile(\"([a-z_]+).*.xml(.bz2)?\");\n+  \n+  protected void listFiles(Path path, FileSystem fs, List<Path> files, Set<String> languages) throws IOException {\n+    for (FileStatus status : fs.listStatus(path)) {\n+      if (status.isDir()) {\n+        listFiles(status.getPath(), fs, files, languages);\n+      } else {\n+        Path p = status.getPath();\n+        Matcher matcher = filePattern.matcher(p.getName());\n+        if (matcher.matches()) {\n+          languages.add(matcher.group(1));\n+          files.add(p);\n+        }\n+      }\n+    }\n+  }\n+}\ndiff --git a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitionedMapper.java b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitionedMapper.java\nnew file mode 100644\nindex 0000000..4d94c24\n--- /dev/null\n+++ b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitionedMapper.java\n@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+/**\n+ * \n+ */\n+package org.apache.accumulo.examples.wikisearch.ingest;\n+\n+\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.nio.charset.Charset;\n+import java.util.HashSet;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+\n+import org.apache.accumulo.core.data.Mutation;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.accumulo.examples.wikisearch.ingest.ArticleExtractor.Article;\n+import org.apache.accumulo.examples.wikisearch.normalizer.LcNoDiacriticsNormalizer;\n+import org.apache.accumulo.examples.wikisearch.protobuf.Uid;\n+import org.apache.accumulo.examples.wikisearch.protobuf.Uid.List.Builder;\n+import org.apache.commons.codec.binary.Base64;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Mapper;\n+import org.apache.log4j.Logger;\n+import org.apache.lucene.analysis.tokenattributes.TermAttribute;\n+import org.apache.lucene.wikipedia.analysis.WikipediaTokenizer;\n+\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.Multimap;\n+\n+public class WikipediaPartitionedMapper extends Mapper<Text,Article,Text,Mutation> {\n+  \n+  private static final Logger log = Logger.getLogger(WikipediaPartitionedMapper.class);\n+  \n+  public final static Charset UTF8 = Charset.forName(\"UTF-8\");\n+  public static final String DOCUMENT_COLUMN_FAMILY = \"d\";\n+  public static final String METADATA_EVENT_COLUMN_FAMILY = \"e\";\n+  public static final String METADATA_INDEX_COLUMN_FAMILY = \"i\";\n+  public static final String TOKENS_FIELD_NAME = \"TEXT\";\n+  \n+  private static final Value NULL_VALUE = new Value(new byte[0]);\n+  private static final String cvPrefix = \"all|\";\n+  \n+  private int numPartitions = 0;\n+\n+  private Text tablename = null;\n+  private Text indexTableName = null;\n+  private Text reverseIndexTableName = null;\n+  private Text metadataTableName = null;\n+  \n+  @Override\n+  public void setup(Context context) {\n+    Configuration conf = context.getConfiguration();\n+    tablename = new Text(WikipediaConfiguration.getTableName(conf));\n+    indexTableName = new Text(tablename + \"Index\");\n+    reverseIndexTableName = new Text(tablename + \"ReverseIndex\");\n+    metadataTableName = new Text(tablename + \"Metadata\");\n+    \n+    numPartitions = WikipediaConfiguration.getNumPartitions(conf);\n+  }\n+  \n+  @Override\n+  protected void map(Text language, Article article, Context context) throws IOException, InterruptedException {\n+    String NULL_BYTE = \"\\u0000\";\n+    String colfPrefix = language.toString() + NULL_BYTE;\n+    String indexPrefix = \"fi\" + NULL_BYTE;\n+    ColumnVisibility cv = new ColumnVisibility(cvPrefix + language);\n+    \n+    if (article != null) {\n+      Text partitionId = new Text(Integer.toString(WikipediaMapper.getPartitionId(article, numPartitions)));\n+      \n+      // Create the mutations for the document.\n+      // Row is partition id, colf is language0articleid, colq is fieldName\\0fieldValue\n+      Mutation m = new Mutation(partitionId);\n+      for (Entry<String,Object> entry : article.getFieldValues().entrySet()) {\n+        m.put(colfPrefix + article.getId(), entry.getKey() + NULL_BYTE + entry.getValue().toString(), cv, article.getTimestamp(), NULL_VALUE);\n+        // Create mutations for the metadata table.\n+        Mutation mm = new Mutation(entry.getKey());\n+        mm.put(METADATA_EVENT_COLUMN_FAMILY, language.toString(), cv, article.getTimestamp(), NULL_VALUE);\n+        context.write(metadataTableName, mm);\n+      }\n+      \n+      // Tokenize the content\n+      Set<String> tokens = getTokens(article);\n+      \n+      // We are going to put the fields to be indexed into a multimap. This allows us to iterate\n+      // over the entire set once.\n+      Multimap<String,String> indexFields = HashMultimap.create();\n+      // Add the normalized field values\n+      LcNoDiacriticsNormalizer normalizer = new LcNoDiacriticsNormalizer();\n+      for (Entry<String,String> index : article.getNormalizedFieldValues().entrySet())\n+        indexFields.put(index.getKey(), index.getValue());\n+      // Add the tokens\n+      for (String token : tokens)\n+        indexFields.put(TOKENS_FIELD_NAME, normalizer.normalizeFieldValue(\"\", token));\n+      \n+      for (Entry<String,String> index : indexFields.entries()) {\n+        // Create mutations for the in partition index\n+        // Row is partition id, colf is 'fi'\\0fieldName, colq is fieldValue\\0language\\0article id\n+        m.put(indexPrefix + index.getKey(), index.getValue() + NULL_BYTE + colfPrefix + article.getId(), cv, article.getTimestamp(), NULL_VALUE);\n+        \n+        // Create mutations for the global index\n+        // Create a UID object for the Value\n+        Builder uidBuilder = Uid.List.newBuilder();\n+        uidBuilder.setIGNORE(false);\n+        uidBuilder.setCOUNT(1);\n+        uidBuilder.addUID(Integer.toString(article.getId()));\n+        Uid.List uidList = uidBuilder.build();\n+        Value val = new Value(uidList.toByteArray());\n+        \n+        // Create mutations for the global index\n+        // Row is field value, colf is field name, colq is partitionid\\0language, value is Uid.List object\n+        Mutation gm = new Mutation(index.getValue());\n+        gm.put(index.getKey(), partitionId + NULL_BYTE + language, cv, article.getTimestamp(), val);\n+        context.write(indexTableName, gm);\n+        \n+        // Create mutations for the global reverse index\n+        Mutation grm = new Mutation(StringUtils.reverse(index.getValue()));\n+        grm.put(index.getKey(), partitionId + NULL_BYTE + language, cv, article.getTimestamp(), val);\n+        context.write(reverseIndexTableName, grm);\n+        \n+        // Create mutations for the metadata table.\n+        Mutation mm = new Mutation(index.getKey());\n+        mm.put(METADATA_INDEX_COLUMN_FAMILY, language + NULL_BYTE + LcNoDiacriticsNormalizer.class.getName(), cv, article.getTimestamp(), NULL_VALUE);\n+        context.write(metadataTableName, mm);\n+        \n+      }\n+      // Add the entire text to the document section of the table.\n+      // row is the partition, colf is 'd', colq is language\\0articleid, value is Base64 encoded GZIP'd document\n+      m.put(DOCUMENT_COLUMN_FAMILY, colfPrefix + article.getId(), cv, article.getTimestamp(), new Value(Base64.encodeBase64(article.getText().getBytes())));\n+      context.write(tablename, m);\n+      \n+    } else {\n+      context.getCounter(\"wikipedia\", \"invalid articles\").increment(1);\n+    }\n+    context.progress();\n+  }\n+  \n+  /**\n+   * Tokenize the wikipedia content\n+   * \n+   * @param article\n+   * @return\n+   * @throws IOException\n+   */\n+  private Set<String> getTokens(Article article) throws IOException {\n+    Set<String> tokenList = new HashSet<String>();\n+    WikipediaTokenizer tok = new WikipediaTokenizer(new StringReader(article.getText()));\n+    TermAttribute term = tok.addAttribute(TermAttribute.class);\n+    try {\n+      while (tok.incrementToken()) {\n+        String token = term.term();\n+        if (!StringUtils.isEmpty(token))\n+          tokenList.add(token);\n+      }\n+    } catch (IOException e) {\n+      log.error(\"Error tokenizing text\", e);\n+    } finally {\n+      try {\n+        tok.end();\n+      } catch (IOException e) {\n+        log.error(\"Error calling end()\", e);\n+      } finally {\n+        try {\n+          tok.close();\n+        } catch (IOException e) {\n+          log.error(\"Error closing tokenizer\", e);\n+        }\n+      }\n+    }\n+    return tokenList;\n+  }\n+  \n+}\ndiff --git a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitioner.java b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitioner.java\nnew file mode 100644\nindex 0000000..82af9fd\n--- /dev/null\n+++ b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitioner.java\n@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+/**\n+ * \n+ */\n+package org.apache.accumulo.examples.wikisearch.ingest;\n+\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.StringReader;\n+import java.nio.charset.Charset;\n+import java.util.HashSet;\n+import java.util.IllegalFormatException;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import org.apache.accumulo.core.data.Mutation;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.accumulo.examples.wikisearch.ingest.ArticleExtractor.Article;\n+import org.apache.accumulo.examples.wikisearch.ingest.WikipediaInputFormat.WikipediaInputSplit;\n+import org.apache.accumulo.examples.wikisearch.normalizer.LcNoDiacriticsNormalizer;\n+import org.apache.accumulo.examples.wikisearch.protobuf.Uid;\n+import org.apache.accumulo.examples.wikisearch.protobuf.Uid.List.Builder;\n+import org.apache.commons.codec.binary.Base64;\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Mapper;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.log4j.Logger;\n+import org.apache.lucene.analysis.tokenattributes.TermAttribute;\n+import org.apache.lucene.wikipedia.analysis.WikipediaTokenizer;\n+\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.Multimap;\n+\n+public class WikipediaPartitioner extends Mapper<LongWritable,Text,Text,Article> {\n+  \n+  private static final Logger log = Logger.getLogger(WikipediaPartitioner.class);\n+  \n+  public final static Charset UTF8 = Charset.forName(\"UTF-8\");\n+  public static final String DOCUMENT_COLUMN_FAMILY = \"d\";\n+  public static final String METADATA_EVENT_COLUMN_FAMILY = \"e\";\n+  public static final String METADATA_INDEX_COLUMN_FAMILY = \"i\";\n+  public static final String TOKENS_FIELD_NAME = \"TEXT\";\n+  \n+  private final static Pattern languagePattern = Pattern.compile(\"([a-z_]+).*.xml(.bz2)?\");\n+  \n+  private ArticleExtractor extractor;\n+  private String language;\n+\n+  private int myGroup = -1;\n+  private int numGroups = -1;\n+  \n+  @Override\n+  public void setup(Context context) {\n+    Configuration conf = context.getConfiguration();\n+    \n+    WikipediaInputSplit wiSplit = (WikipediaInputSplit)context.getInputSplit();\n+    myGroup = wiSplit.getPartition();\n+    numGroups = WikipediaConfiguration.getNumGroups(conf);\n+    \n+    FileSplit split = wiSplit.getFileSplit();\n+    String fileName = split.getPath().getName();\n+    Matcher matcher = languagePattern.matcher(fileName);\n+    if (matcher.matches()) {\n+      language = matcher.group(1).replace('_', '-').toLowerCase();\n+    } else {\n+      throw new RuntimeException(\"Unknown ingest language! \" + fileName);\n+    }\n+    extractor = new ArticleExtractor();\n+  }\n+  \n+  @Override\n+  protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n+    Article article = extractor.extract(new InputStreamReader(new ByteArrayInputStream(value.getBytes()), UTF8));\n+    if (article != null) {\n+      int groupId = WikipediaMapper.getPartitionId(article, numGroups);\n+      if(groupId != myGroup)\n+        return;\n+      context.write(new Text(language), article);\n+    } else {\n+      context.getCounter(\"wikipedia\", \"invalid articles\").increment(1);\n+      context.progress();\n+    }\n+  }\n+  \n+}\ndiff --git a/src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java b/src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java\nindex 3e719e6..e709704 100644\n--- a/src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java\n+++ b/src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java\n@@ -123,6 +123,8 @@ import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n import org.apache.log4j.Logger;\n+import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.KeeperException.NoNodeException;\n \n import cloudtrace.instrument.Span;\n import cloudtrace.instrument.Trace;\n@@ -2274,6 +2276,7 @@ public class Tablet {\n       if (updateMetadata) {\n         synchronized (this) {\n           updatingFlushID = false;\n+          this.notifyAll();\n         }\n       }\n     }\n@@ -2281,8 +2284,19 @@ public class Tablet {\n   }\n   \n   boolean initiateMinorCompaction() {\n+    if (isClosed()) {\n+      // don't bother trying to get flush id if closed... could be closed after this check but that is ok... just trying to cut down on uneeded log messages....\n+      return false;\n+    }\n+\n     // get the flush id before the new memmap is made available for write\n-    long flushId = getFlushID();\n+    long flushId;\n+    try {\n+      flushId = getFlushID();\n+    } catch (NoNodeException e) {\n+      log.info(\"Asked to initiate MinC when there was no flush id \" + getExtent() + \" \" + e.getMessage());\n+      return false;\n+    }\n     return initiateMinorCompaction(flushId);\n   }\n   \n@@ -2338,23 +2352,39 @@ public class Tablet {\n     return true;\n   }\n   \n-  long getFlushID() {\n+  long getFlushID() throws NoNodeException {\n     try {\n       String zTablePath = Constants.ZROOT + \"/\" + HdfsZooInstance.getInstance().getInstanceID() + Constants.ZTABLES + \"/\" + extent.getTableId()\n           + Constants.ZTABLE_FLUSH_ID;\n       return Long.parseLong(new String(ZooReaderWriter.getRetryingInstance().getData(zTablePath, null)));\n-    } catch (Exception e) {\n+    } catch (InterruptedException e) {\n       throw new RuntimeException(e);\n+    } catch (NumberFormatException nfe) {\n+      throw new RuntimeException(nfe);\n+    } catch (KeeperException ke) {\n+      if (ke instanceof NoNodeException) {\n+        throw (NoNodeException) ke;\n+      } else {\n+        throw new RuntimeException(ke);\n+      }\n     }\n   }\n   \n-  long getCompactionID() {\n+  long getCompactionID() throws NoNodeException {\n     try {\n       String zTablePath = Constants.ZROOT + \"/\" + HdfsZooInstance.getInstance().getInstanceID() + Constants.ZTABLES + \"/\" + extent.getTableId()\n           + Constants.ZTABLE_COMPACT_ID;\n       return Long.parseLong(new String(ZooReaderWriter.getRetryingInstance().getData(zTablePath, null)));\n-    } catch (Exception e) {\n+    } catch (InterruptedException e) {\n       throw new RuntimeException(e);\n+    } catch (NumberFormatException nfe) {\n+      throw new RuntimeException(nfe);\n+    } catch (KeeperException ke) {\n+      if (ke instanceof NoNodeException) {\n+        throw (NoNodeException) ke;\n+      } else {\n+        throw new RuntimeException(ke);\n+      }\n     }\n   }\n   \n@@ -2557,13 +2587,25 @@ public class Tablet {\n         }\n       }\n       \n+      while (updatingFlushID) {\n+        try {\n+          this.wait(50);\n+        } catch (InterruptedException e) {\n+          log.error(e.toString());\n+        }\n+      }\n+\n       if (!saveState || tabletMemory.getMemTable().getNumEntries() == 0) {\n         return;\n       }\n       \n       tabletMemory.waitForMinC();\n       \n-      mct = prepareForMinC(getFlushID());\n+      try {\n+        mct = prepareForMinC(getFlushID());\n+      } catch (NoNodeException e) {\n+        throw new RuntimeException(e);\n+      }\n       \n       if (queueMinC) {\n         tabletResources.executeMinorCompaction(mct);\n@@ -2612,7 +2654,11 @@ public class Tablet {\n     tabletMemory.waitForMinC();\n     \n     if (saveState && tabletMemory.getMemTable().getNumEntries() > 0) {\n-      prepareForMinC(getFlushID()).run();\n+      try {\n+        prepareForMinC(getFlushID()).run();\n+      } catch (NoNodeException e) {\n+        throw new RuntimeException(e);\n+      }\n     }\n     \n     if (saveState) {\n@@ -3103,7 +3149,11 @@ public class Tablet {\n       Long compactionId = null;\n       if (!propogateDeletes) {\n         // compacting everything, so update the compaction id in !METADATA\n-        compactionId = getCompactionID();\n+        try {\n+          compactionId = getCompactionID();\n+        } catch (NoNodeException e) {\n+          throw new RuntimeException(e);\n+        }\n       }\n       \n       // need to handle case where only one file is being major compacted\ndiff --git a/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletServer.java b/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletServer.java\nindex e01ca07..94e8137 100644\n--- a/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletServer.java\n+++ b/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletServer.java\n@@ -194,6 +194,7 @@ import org.apache.thrift.TProcessor;\n import org.apache.thrift.TServiceClient;\n import org.apache.thrift.server.TServer;\n import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.KeeperException.NoNodeException;\n \n import cloudtrace.instrument.Span;\n import cloudtrace.instrument.Trace;\n@@ -1887,7 +1888,13 @@ public class TabletServer extends AbstractMetricsImpl implements org.apache.accu\n         if (flushID == null) {\n           // read the flush id once from zookeeper instead of reading\n           // it for each tablet\n-          flushID = tablet.getFlushID();\n+          try {\n+            flushID = tablet.getFlushID();\n+          } catch (NoNodeException e) {\n+            // table was probably deleted\n+            log.info(\"Asked to flush table that has no flush id \" + ke + \" \" + e.getMessage());\n+            return;\n+          }\n         }\n         tablet.flush(flushID);\n       }\n@@ -1904,7 +1911,11 @@ public class TabletServer extends AbstractMetricsImpl implements org.apache.accu\n       Tablet tablet = onlineTablets.get(new KeyExtent(textent));\n       if (tablet != null) {\n         log.info(\"Flushing \" + tablet.getExtent());\n-        tablet.flush(tablet.getFlushID());\n+        try {\n+          tablet.flush(tablet.getFlushID());\n+        } catch (NoNodeException nne) {\n+          log.info(\"Asked to flush tablet that has no flush id \" + new KeyExtent(textent) + \" \" + nne.getMessage());\n+        }\n       }\n     }\n     \n@@ -1999,7 +2010,12 @@ public class TabletServer extends AbstractMetricsImpl implements org.apache.accu\n         // all for the same table id, so only need to read\n         // compaction id once\n         if (compactionId == null)\n-          compactionId = tablet.getCompactionID();\n+          try {\n+            compactionId = tablet.getCompactionID();\n+          } catch (NoNodeException e) {\n+            log.info(\"Asked to compact table with no compaction id \" + ke + \" \" + e.getMessage());\n+            return;\n+          }\n         tablet.compactAll(compactionId);\n       }\n       \n", 
  "project": "accumulo", 
  "linesAdd": 685, 
  "nb_skipped": 0, 
  "fix_commit": "db4a291f", 
  "nb_failure": 0, 
  "id": "366", 
  "linesRem": 20
}