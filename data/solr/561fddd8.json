{
  "project": "solr",
  "jira_id": "5908",
  "commit": "561fddd8",
  "classification": {
    "singleLine": false
  },
  "patch": "diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenizer.java\nindex 5bb12d402c..97895604e0 100644\n--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenizer.java\n+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenizer.java\n@@ -17,18 +17,105 @@ package org.apache.lucene.analysis.ngram;\n  * limitations under the License.\n  */\n \n+import java.io.IOException;\n+\n+import org.apache.lucene.analysis.Tokenizer;\n+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;\n+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;\n+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;\n+import org.apache.lucene.util.ArrayUtil;\n import org.apache.lucene.util.AttributeFactory;\n+import org.apache.lucene.util.Version;\n \n /**\n- * Tokenizes the input from an edge into n-grams of given size(s), using pre-4.4 behavior.\n- *\n- * @deprecated Use {@link org.apache.lucene.analysis.ngram.EdgeNGramTokenizer}.\n+ * Old version of {@link EdgeNGramTokenizer} which doesn't handle correctly\n+ * supplementary characters.\n  */\n @Deprecated\n-public class Lucene43EdgeNGramTokenizer extends Lucene43NGramTokenizer {\n+public final class Lucene43EdgeNGramTokenizer extends Tokenizer {\n+  public static final Side DEFAULT_SIDE = Side.FRONT;\n   public static final int DEFAULT_MAX_GRAM_SIZE = 1;\n   public static final int DEFAULT_MIN_GRAM_SIZE = 1;\n \n+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n+  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n+  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n+\n+  /** Specifies which side of the input the n-gram should be generated from */\n+  public static enum Side {\n+\n+    /** Get the n-gram from the front of the input */\n+    FRONT {\n+      @Override\n+      public String getLabel() { return \"front\"; }\n+    },\n+\n+    /** Get the n-gram from the end of the input */\n+    BACK  {\n+      @Override\n+      public String getLabel() { return \"back\"; }\n+    };\n+\n+    public abstract String getLabel();\n+\n+    // Get the appropriate Side from a string\n+    public static Side getSide(String sideName) {\n+      if (FRONT.getLabel().equals(sideName)) {\n+        return FRONT;\n+      }\n+      if (BACK.getLabel().equals(sideName)) {\n+        return BACK;\n+      }\n+      return null;\n+    }\n+  }\n+\n+  private int minGram;\n+  private int maxGram;\n+  private int gramSize;\n+  private Side side;\n+  private boolean started;\n+  private int inLen; // length of the input AFTER trim()\n+  private int charsRead; // length of the input\n+  private String inStr;\n+\n+\n+  /**\n+   * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range\n+   *\n+   * @param side the {@link Side} from which to chop off an n-gram\n+   * @param minGram the smallest n-gram to generate\n+   * @param maxGram the largest n-gram to generate\n+   */\n+  public Lucene43EdgeNGramTokenizer(Side side, int minGram, int maxGram) {\n+    init(side, minGram, maxGram);\n+  }\n+\n+  /**\n+   * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range\n+   *\n+   * @param factory {@link org.apache.lucene.util.AttributeFactory} to use\n+   * @param side the {@link Side} from which to chop off an n-gram\n+   * @param minGram the smallest n-gram to generate\n+   * @param maxGram the largest n-gram to generate\n+   */\n+  public Lucene43EdgeNGramTokenizer(AttributeFactory factory, Side side, int minGram, int maxGram) {\n+    super(factory);\n+    init(side, minGram, maxGram);\n+  }\n+\n+  /**\n+   * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range\n+   *\n+   * @param factory {@link org.apache.lucene.util.AttributeFactory} to use\n+   * @param sideLabel the {@link Side} from which to chop off an n-gram\n+   * @param minGram the smallest n-gram to generate\n+   * @param maxGram the largest n-gram to generate\n+   */\n+  public Lucene43EdgeNGramTokenizer(AttributeFactory factory, String sideLabel, int minGram, int maxGram) {\n+    this(factory, Side.getSide(sideLabel), minGram, maxGram);\n+  }\n+\n   /**\n    * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range\n    *\n@@ -36,7 +123,19 @@ public class Lucene43EdgeNGramTokenizer extends Lucene43NGramTokenizer {\n    * @param maxGram the largest n-gram to generate\n    */\n   public Lucene43EdgeNGramTokenizer(int minGram, int maxGram) {\n-    super(minGram, maxGram);\n+    this(Side.FRONT, minGram, maxGram);\n+  }\n+\n+  /**\n+   * Creates EdgeNGramTokenizer that can generate n-grams in the sizes of the given range\n+   *\n+   * @param sideLabel the name of the {@link Side} from which to chop off an n-gram\n+   * @param minGram the smallest n-gram to generate\n+   * @param maxGram the largest n-gram to generate\n+   */\n+  @Deprecated\n+  public Lucene43EdgeNGramTokenizer(String sideLabel, int minGram, int maxGram) {\n+    this(Side.getSide(sideLabel), minGram, maxGram);\n   }\n \n   /**\n@@ -47,7 +146,110 @@ public class Lucene43EdgeNGramTokenizer extends Lucene43NGramTokenizer {\n    * @param maxGram the largest n-gram to generate\n    */\n   public Lucene43EdgeNGramTokenizer(AttributeFactory factory, int minGram, int maxGram) {\n-    super(factory, minGram, maxGram);\n+    this(factory, Side.FRONT, minGram, maxGram);\n+  }\n+\n+  private void init(Side side, int minGram, int maxGram) {\n+\n+    if (side == null) {\n+      throw new IllegalArgumentException(\"sideLabel must be either front or back\");\n+    }\n+\n+    if (minGram < 1) {\n+      throw new IllegalArgumentException(\"minGram must be greater than zero\");\n+    }\n+\n+    if (minGram > maxGram) {\n+      throw new IllegalArgumentException(\"minGram must not be greater than maxGram\");\n+    }\n+\n+    maxGram = Math.min(maxGram, 1024);\n+\n+    this.minGram = minGram;\n+    this.maxGram = maxGram;\n+    this.side = side;\n+  }\n+\n+  /** Returns the next token in the stream, or null at EOS. */\n+  @Override\n+  public boolean incrementToken() throws IOException {\n+    clearAttributes();\n+    // if we are just starting, read the whole input\n+    if (!started) {\n+      started = true;\n+      gramSize = minGram;\n+      final int limit = side == Side.FRONT ? maxGram : 1024;\n+      char[] chars = new char[Math.min(1024, limit)];\n+      charsRead = 0;\n+      // TODO: refactor to a shared readFully somewhere:\n+      boolean exhausted = false;\n+      while (charsRead < limit) {\n+        final int inc = input.read(chars, charsRead, chars.length-charsRead);\n+        if (inc == -1) {\n+          exhausted = true;\n+          break;\n+        }\n+        charsRead += inc;\n+        if (charsRead == chars.length && charsRead < limit) {\n+          chars = ArrayUtil.grow(chars);\n+        }\n+      }\n+\n+      inStr = new String(chars, 0, charsRead);\n+      inStr = inStr.trim();\n+\n+      if (!exhausted) {\n+        // Read extra throwaway chars so that on end() we\n+        // report the correct offset:\n+        char[] throwaway = new char[1024];\n+        while(true) {\n+          final int inc = input.read(throwaway, 0, throwaway.length);\n+          if (inc == -1) {\n+            break;\n+          }\n+          charsRead += inc;\n+        }\n+      }\n+\n+      inLen = inStr.length();\n+      if (inLen == 0) {\n+        return false;\n+      }\n+      posIncrAtt.setPositionIncrement(1);\n+    } else {\n+      posIncrAtt.setPositionIncrement(0);\n+    }\n+\n+    // if the remaining input is too short, we can't generate any n-grams\n+    if (gramSize > inLen) {\n+      return false;\n+    }\n+\n+    // if we have hit the end of our n-gram size range, quit\n+    if (gramSize > maxGram || gramSize > inLen) {\n+      return false;\n     }\n \n+    // grab gramSize chars from front or back\n+    int start = side == Side.FRONT ? 0 : inLen - gramSize;\n+    int end = start + gramSize;\n+    termAtt.setEmpty().append(inStr, start, end);\n+    offsetAtt.setOffset(correctOffset(start), correctOffset(end));\n+    gramSize++;\n+    return true;\n+  }\n+  \n+  @Override\n+  public void end() throws IOException {\n+    super.end();\n+    // set final offset\n+    final int finalOffset = correctOffset(charsRead);\n+    this.offsetAtt.setOffset(finalOffset, finalOffset);\n+  }    \n+\n+  @Override\n+  public void reset() throws IOException {\n+    super.reset();\n+    started = false;\n+  }\n }\ndiff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenizer.java\nindex 8cde3e40c6..b1d007a0dd 100644\n--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenizer.java\n+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenizer.java\n@@ -18,7 +18,7 @@ package org.apache.lucene.analysis.ngram;\n  */\n \n import java.io.IOException;\n-import java.io.Reader;\n+\n \n import org.apache.lucene.analysis.Tokenizer;\n import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;\n@@ -29,7 +29,7 @@ import org.apache.lucene.util.AttributeFactory;\n  * Old broken version of {@link NGramTokenizer}.\n  */\n @Deprecated\n-public class Lucene43NGramTokenizer extends Tokenizer {\n+public final class Lucene43NGramTokenizer extends Tokenizer {\n   public static final int DEFAULT_MIN_NGRAM_SIZE = 1;\n   public static final int DEFAULT_MAX_NGRAM_SIZE = 2;\n \n",
  "files": 2,
  "linesAdd": 210,
  "linesRem": 8,
  "failing_tests": [],
  "nb_test": 0,
  "nb_failure": 0,
  "nb_error": 0,
  "nb_skipped": 0
}