{
  "project": "nifi",
  "jira_id": "2670",
  "commit": "102a9a2b",
  "classification": {
    "singleLine": false
  },
  "patch": "diff --git a/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/ConsumeKafka.java b/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/ConsumeKafka.java\nindex e5255f5764..0a3fe5d969 100644\n--- a/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/ConsumeKafka.java\n+++ b/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/ConsumeKafka.java\n@@ -62,7 +62,8 @@ import static org.apache.nifi.processors.kafka.pubsub.KafkaProcessorUtils.SECURI\n @Tags({\"Kafka\", \"Get\", \"Ingest\", \"Ingress\", \"Topic\", \"PubSub\", \"Consume\", \"0.9.x\"})\n @WritesAttributes({\n     @WritesAttribute(attribute = KafkaProcessorUtils.KAFKA_COUNT, description = \"The number of messages written if more than one\"),\n-    @WritesAttribute(attribute = KafkaProcessorUtils.KAFKA_KEY_HEX, description = \"The hex encoded key of message if present and if single message\"),\n+    @WritesAttribute(attribute = KafkaProcessorUtils.KAFKA_KEY, description = \"The key of message if present and if single message. \"\n+        + \"How the key is encoded depends on the value of the 'Key Attribute Encoding' property.\"),\n     @WritesAttribute(attribute = KafkaProcessorUtils.KAFKA_OFFSET, description = \"The offset of the message in the partition of the topic.\"),\n     @WritesAttribute(attribute = KafkaProcessorUtils.KAFKA_PARTITION, description = \"The partition of the topic the message or message bundle is from\"),\n     @WritesAttribute(attribute = KafkaProcessorUtils.KAFKA_TOPIC, description = \"The topic the message or message bundle is from\")\n@@ -82,6 +83,10 @@ public class ConsumeKafka extends AbstractProcessor {\n \n     static final AllowableValue OFFSET_NONE = new AllowableValue(\"none\", \"none\", \"Throw exception to the consumer if no previous offset is found for the consumer's group\");\n \n+    static final AllowableValue UTF8_ENCODING = new AllowableValue(\"utf-8\", \"UTF-8 Encoded\", \"The key is interpreted as a UTF-8 Encoded string.\");\n+    static final AllowableValue HEX_ENCODING = new AllowableValue(\"hex\", \"Hex Encoded\",\n+        \"The key is interpreted as arbitrary binary data and is encoded using hexadecimal characters with uppercase letters\");\n+\n     static final PropertyDescriptor TOPICS = new PropertyDescriptor.Builder()\n             .name(\"topic\")\n             .displayName(\"Topic Name(s)\")\n@@ -110,6 +115,15 @@ public class ConsumeKafka extends AbstractProcessor {\n             .defaultValue(OFFSET_LATEST.getValue())\n             .build();\n \n+    static final PropertyDescriptor KEY_ATTRIBUTE_ENCODING = new PropertyDescriptor.Builder()\n+            .name(\"key-attribute-encoding\")\n+            .displayName(\"Key Attribute Encoding\")\n+            .description(\"FlowFiles that are emitted have an attribute named '\" + KafkaProcessorUtils.KAFKA_KEY + \"'. This property dictates how the value of the attribute should be encoded.\")\n+            .required(true)\n+            .defaultValue(UTF8_ENCODING.getValue())\n+            .allowableValues(UTF8_ENCODING, HEX_ENCODING)\n+            .build();\n+\n     static final PropertyDescriptor MESSAGE_DEMARCATOR = new PropertyDescriptor.Builder()\n             .name(\"message-demarcator\")\n             .displayName(\"Message Demarcator\")\n@@ -148,6 +162,7 @@ public class ConsumeKafka extends AbstractProcessor {\n         descriptors.add(TOPICS);\n         descriptors.add(GROUP_ID);\n         descriptors.add(AUTO_OFFSET_RESET);\n+        descriptors.add(KEY_ATTRIBUTE_ENCODING);\n         descriptors.add(MESSAGE_DEMARCATOR);\n         descriptors.add(MAX_POLL_RECORDS);\n         DESCRIPTORS = Collections.unmodifiableList(descriptors);\n@@ -290,10 +305,24 @@ public class ConsumeKafka extends AbstractProcessor {\n         }\n     }\n \n+    private String encodeKafkaKey(final byte[] key, final String encoding) {\n+        if (key == null) {\n+            return null;\n+        }\n+\n+        if (HEX_ENCODING.getValue().equals(encoding)) {\n+            return DatatypeConverter.printHexBinary(key);\n+        } else if (UTF8_ENCODING.getValue().equals(encoding)) {\n+            return new String(key, StandardCharsets.UTF_8);\n+        } else {\n+            return null;    // won't happen because it is guaranteed by the Allowable Values\n+        }\n+    }\n+\n     private void writeData(final ProcessContext context, final ProcessSession session, final List<ConsumerRecord<byte[], byte[]>> records, final long startTimeNanos) {\n         final ConsumerRecord<byte[], byte[]> firstRecord = records.get(0);\n         final String offset = String.valueOf(firstRecord.offset());\n-        final String keyHex = (firstRecord.key() != null) ? DatatypeConverter.printHexBinary(firstRecord.key()) : null;\n+        final String keyValue = encodeKafkaKey(firstRecord.key(), context.getProperty(KEY_ATTRIBUTE_ENCODING).getValue());\n         final String topic = firstRecord.topic();\n         final String partition = String.valueOf(firstRecord.partition());\n         FlowFile flowFile = session.create();\n@@ -309,8 +338,8 @@ public class ConsumeKafka extends AbstractProcessor {\n         });\n         final Map<String, String> kafkaAttrs = new HashMap<>();\n         kafkaAttrs.put(KafkaProcessorUtils.KAFKA_OFFSET, offset);\n-        if (keyHex != null && records.size() == 1) {\n-            kafkaAttrs.put(KafkaProcessorUtils.KAFKA_KEY_HEX, keyHex);\n+        if (keyValue != null && records.size() == 1) {\n+            kafkaAttrs.put(KafkaProcessorUtils.KAFKA_KEY, keyValue);\n         }\n         kafkaAttrs.put(KafkaProcessorUtils.KAFKA_PARTITION, partition);\n         kafkaAttrs.put(KafkaProcessorUtils.KAFKA_TOPIC, topic);\ndiff --git a/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/KafkaProcessorUtils.java b/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/KafkaProcessorUtils.java\nindex fd747fca02..c74ad18596 100644\n--- a/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/KafkaProcessorUtils.java\n+++ b/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/KafkaProcessorUtils.java\n@@ -57,7 +57,7 @@ final class KafkaProcessorUtils {\n \n     static final Pattern HEX_KEY_PATTERN = Pattern.compile(\"(?:[0123456789abcdefABCDEF]{2})+\");\n \n-    static final String KAFKA_KEY_HEX = \"kafka.key.hex\";\n+    static final String KAFKA_KEY = \"kafka.key\";\n     static final String KAFKA_TOPIC = \"kafka.topic\";\n     static final String KAFKA_PARTITION = \"kafka.partition\";\n     static final String KAFKA_OFFSET = \"kafka.offset\";\ndiff --git a/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/PublishKafka.java b/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/PublishKafka.java\nindex 65f386e8a8..4e1403dbf4 100644\n--- a/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/PublishKafka.java\n+++ b/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/PublishKafka.java\n@@ -104,6 +104,10 @@ public class PublishKafka extends AbstractSessionFactoryProcessor {\n     static final AllowableValue RANDOM_PARTITIONING = new AllowableValue(\"org.apache.kafka.clients.producer.internals.DefaultPartitioner\",\n             \"DefaultPartitioner\", \"Messages will be assigned to random partitions.\");\n \n+    static final AllowableValue UTF8_ENCODING = new AllowableValue(\"utf-8\", \"UTF-8 Encoded\", \"The key is interpreted as a UTF-8 Encoded string.\");\n+    static final AllowableValue HEX_ENCODING = new AllowableValue(\"hex\", \"Hex Encoded\",\n+        \"The key is interpreted as arbitrary binary data that is encoded using hexadecimal characters with uppercase letters.\");\n+\n     static final PropertyDescriptor TOPIC = new PropertyDescriptor.Builder()\n             .name(\"topic\")\n             .displayName(\"Topic Name\")\n@@ -146,15 +150,23 @@ public class PublishKafka extends AbstractSessionFactoryProcessor {\n     static final PropertyDescriptor KEY = new PropertyDescriptor.Builder()\n             .name(\"kafka-key\")\n             .displayName(\"Kafka Key\")\n-            .description(\"The Key to use for the Message.  It will be serialized as UTF-8 bytes. \"\n-                    + \"If not specified then the flow file attribute kafka.key.hex is used if present \"\n-                    + \"and we're not demarcating. In that case the hex string is coverted to its byte\"\n-                    + \"form and written as a byte[] key.\")\n+            .description(\"The Key to use for the Message. \"\n+                    + \"If not specified, the flow file attribute 'kafka.key' is used as the message key, if it is present \"\n+                    + \"and we're not demarcating.\")\n             .required(false)\n             .addValidator(StandardValidators.NON_EMPTY_VALIDATOR)\n             .expressionLanguageSupported(true)\n             .build();\n \n+    static final PropertyDescriptor KEY_ATTRIBUTE_ENCODING = new PropertyDescriptor.Builder()\n+            .name(\"key-attribute-encoding\")\n+            .displayName(\"Key Attribute Encoding\")\n+            .description(\"FlowFiles that are emitted have an attribute named '\" + KafkaProcessorUtils.KAFKA_KEY + \"'. This property dictates how the value of the attribute should be encoded.\")\n+            .required(true)\n+            .defaultValue(UTF8_ENCODING.getValue())\n+            .allowableValues(UTF8_ENCODING, HEX_ENCODING)\n+            .build();\n+\n     static final PropertyDescriptor MESSAGE_DEMARCATOR = new PropertyDescriptor.Builder()\n             .name(\"message-demarcator\")\n             .displayName(\"Message Demarcator\")\n@@ -216,6 +228,7 @@ public class PublishKafka extends AbstractSessionFactoryProcessor {\n         _descriptors.add(TOPIC);\n         _descriptors.add(DELIVERY_GUARANTEE);\n         _descriptors.add(KEY);\n+        _descriptors.add(KEY_ATTRIBUTE_ENCODING);\n         _descriptors.add(MESSAGE_DEMARCATOR);\n         _descriptors.add(MAX_REQUEST_SIZE);\n         _descriptors.add(META_WAIT_TIME);\n@@ -449,26 +462,18 @@ public class PublishKafka extends AbstractSessionFactoryProcessor {\n      * regardless if it has #FAILED* attributes set.\n      */\n     private PublishingContext buildPublishingContext(FlowFile flowFile, ProcessContext context, InputStream contentStream) {\n-        String topicName;\n-        byte[] keyBytes;\n-        byte[] delimiterBytes = null;\n+        final byte[] keyBytes = getMessageKey(flowFile, context);\n+\n+        final String topicName;\n+        final byte[] delimiterBytes;\n         int lastAckedMessageIndex = -1;\n         if (this.isFailedFlowFile(flowFile)) {\n             lastAckedMessageIndex = Integer.valueOf(flowFile.getAttribute(FAILED_LAST_ACK_IDX));\n             topicName = flowFile.getAttribute(FAILED_TOPIC_ATTR);\n-            keyBytes = flowFile.getAttribute(FAILED_KEY_ATTR) != null\n-                    ? flowFile.getAttribute(FAILED_KEY_ATTR).getBytes(StandardCharsets.UTF_8) : null;\n             delimiterBytes = flowFile.getAttribute(FAILED_DELIMITER_ATTR) != null\n                     ? flowFile.getAttribute(FAILED_DELIMITER_ATTR).getBytes(StandardCharsets.UTF_8) : null;\n-\n         } else {\n             topicName = context.getProperty(TOPIC).evaluateAttributeExpressions(flowFile).getValue();\n-            String _key = context.getProperty(KEY).evaluateAttributeExpressions(flowFile).getValue();\n-            keyBytes = _key == null ? null : _key.getBytes(StandardCharsets.UTF_8);\n-            String keyHex = flowFile.getAttribute(KafkaProcessorUtils.KAFKA_KEY_HEX);\n-            if (_key == null && keyHex != null && KafkaProcessorUtils.HEX_KEY_PATTERN.matcher(keyHex).matches()) {\n-                keyBytes = DatatypeConverter.parseHexBinary(keyHex);\n-            }\n             delimiterBytes = context.getProperty(MESSAGE_DEMARCATOR).isSet() ? context.getProperty(MESSAGE_DEMARCATOR)\n                     .evaluateAttributeExpressions(flowFile).getValue().getBytes(StandardCharsets.UTF_8) : null;\n         }\n@@ -480,6 +485,26 @@ public class PublishKafka extends AbstractSessionFactoryProcessor {\n         return publishingContext;\n     }\n \n+    private byte[] getMessageKey(final FlowFile flowFile, final ProcessContext context) {\n+        final String uninterpretedKey;\n+        if (context.getProperty(KEY).isSet()) {\n+            uninterpretedKey = context.getProperty(KEY).evaluateAttributeExpressions(flowFile).getValue();\n+        } else {\n+            uninterpretedKey = flowFile.getAttribute(KafkaProcessorUtils.KAFKA_KEY);\n+        }\n+\n+        if (uninterpretedKey == null) {\n+            return null;\n+        }\n+\n+        final String keyEncoding = context.getProperty(KEY_ATTRIBUTE_ENCODING).getValue();\n+        if (UTF8_ENCODING.getValue().equals(keyEncoding)) {\n+            return uninterpretedKey.getBytes(StandardCharsets.UTF_8);\n+        }\n+\n+        return DatatypeConverter.parseHexBinary(uninterpretedKey);\n+    }\n+\n     /**\n      * Will remove FAILED_* attributes if FlowFile is no longer considered a\n      * failed FlowFile\n",
  "files": 3,
  "linesAdd": 75,
  "linesRem": 21,
  "failing_tests": [],
  "nb_test": 0,
  "nb_failure": 0,
  "nb_error": 0,
  "nb_skipped": 0
}